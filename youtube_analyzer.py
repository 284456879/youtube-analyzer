#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YouTubeè§†é¢‘çƒ­åº¦åˆ†æå·¥å…·
åŠŸèƒ½ï¼šè‡ªåŠ¨åˆ†æYouTubeè§†é¢‘æ•°æ®ï¼Œç­›é€‰é«˜çƒ­åº¦å†…å®¹ï¼Œå¯¼å‡ºExcelæŠ¥è¡¨
"""

import os
import json
import re
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import pandas as pd
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ç¡®ä¿æ§åˆ¶å°è¾“å‡ºä½¿ç”¨UTF-8ï¼Œé¿å…emojiæ‰“å°æŠ¥é”™
try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8", errors="ignore")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8", errors="ignore")
except Exception:
    pass

class YouTubeAnalyzer:
    """YouTubeè§†é¢‘åˆ†æå™¨"""
    
    def __init__(self, api_key: str, cpm_low: float = 2.0, cpm_high: float = 4.0,
                 default_language: str = "en", default_region_code: str = "US"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            api_key: YouTube Data API v3 å¯†é’¥
            cpm_low: é¢„ä¼°æ¯åƒæ¬¡æ’­æ”¾CPMä¸‹é™ï¼ˆç¾å…ƒï¼‰
            cpm_high: é¢„ä¼°æ¯åƒæ¬¡æ’­æ”¾CPMä¸Šé™ï¼ˆç¾å…ƒï¼‰
        """
        self.api_key = api_key
        self.youtube = build('youtube', 'v3', developerKey=api_key)
        self.videos_data = []
        self.cpm_low = cpm_low
        self.cpm_high = cpm_high
        self.hot_keywords = [
            'hack', 'hacks', 'diy', 'tips', 'trick', 'tricks', 'challenge', 'viral',
            'recipe', 'cook', 'cooking', 'air fryer', 'slime', 'asmr', 'shortcut',
            'easy', 'fast', 'quick', 'life', 'tiktok', 'shorts'
        ]
        self.trend_window_days = 14
        self.default_language = default_language
        self.default_region_code = default_region_code
        
    def search_videos(self, keyword: str, max_results: int = 50,
                      language: Optional[str] = None,
                      region: Optional[str] = None) -> List[str]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢è§†é¢‘ï¼ˆé’ˆå¯¹æ¬§ç¾åœ°åŒºçƒ­é—¨å†…å®¹ï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_results: è¿”å›ç»“æœæ•°é‡ï¼ˆæœ€å¤š50ï¼‰
            
        Returns:
            è§†é¢‘IDåˆ—è¡¨
        """
        try:
            # æœç´¢æœ€è¿‘14å¤©å†…çš„è§†é¢‘ï¼ˆæ›´æ–°é²œçš„å†…å®¹ï¼‰
            published_after = (datetime.now() - timedelta(days=14)).isoformat() + 'Z'
            
            params = {
                "part": "id",
                "q": keyword,
                "type": "video",
                "order": "viewCount",
                "maxResults": max_results,
                "publishedAfter": published_after,
                "regionCode": (region or self.default_region_code),
                "videoDuration": "medium"
            }
            lang = language or self.default_language
            if lang:
                params["relevanceLanguage"] = lang
            request = self.youtube.search().list(**params)
            response = request.execute()
            
            video_ids = [item['id']['videoId'] for item in response.get('items', [])]
            print(f"âœ… æ‰¾åˆ° {len(video_ids)} ä¸ªæ¬§ç¾åœ°åŒºç›¸å…³è§†é¢‘")
            return video_ids
            
        except HttpError as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_channel_videos(self, channel_url: str, max_results: int = 50) -> List[str]:
        """
        è·å–é¢‘é“çš„è§†é¢‘åˆ—è¡¨
        
        Args:
            channel_url: YouTubeé¢‘é“URL
            max_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            è§†é¢‘IDåˆ—è¡¨
        """
        try:
            # æå–é¢‘é“ID
            channel_id = self._extract_channel_id(channel_url)
            if not channel_id:
                print("âŒ æ— æ•ˆçš„é¢‘é“URL")
                return []
            
            # è·å–é¢‘é“çš„uploadsæ’­æ”¾åˆ—è¡¨
            request = self.youtube.channels().list(
                part="contentDetails",
                id=channel_id
            )
            response = request.execute()
            
            if not response.get('items'):
                print("âŒ æ‰¾ä¸åˆ°è¯¥é¢‘é“")
                return []
            
            uploads_playlist_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
            
            # è·å–æ’­æ”¾åˆ—è¡¨ä¸­çš„è§†é¢‘
            video_ids = []
            next_page_token = None
            
            while len(video_ids) < max_results:
                request = self.youtube.playlistItems().list(
                    part="contentDetails",
                    playlistId=uploads_playlist_id,
                    maxResults=min(50, max_results - len(video_ids)),
                    pageToken=next_page_token
                )
                response = request.execute()
                
                video_ids.extend([item['contentDetails']['videoId'] for item in response.get('items', [])])
                
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break
            
            print(f"âœ… ä»é¢‘é“è·å– {len(video_ids)} ä¸ªè§†é¢‘")
            return video_ids
            
        except HttpError as e:
            print(f"âŒ è·å–é¢‘é“è§†é¢‘å¤±è´¥: {e}")
            return []
    
    def _extract_channel_id(self, channel_url: str) -> Optional[str]:
        """æå–é¢‘é“ID"""
        # åŒ¹é… @username æ ¼å¼
        if '@' in channel_url:
            username = channel_url.split('@')[-1].split('/')[0]
            try:
                request = self.youtube.channels().list(
                    part="id",
                    forHandle=username
                )
                response = request.execute()
                if response.get('items'):
                    return response['items'][0]['id']
            except:
                pass
        
        # åŒ¹é… channel/ID æ ¼å¼
        match = re.search(r'channel/([a-zA-Z0-9_-]+)', channel_url)
        if match:
            return match.group(1)
        
        # ç›´æ¥æ˜¯ID
        if re.match(r'^[a-zA-Z0-9_-]{24}$', channel_url):
            return channel_url
        
        return None
    
    def get_video_details(self, video_ids: List[str]) -> List[Dict]:
        """
        è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
        
        Args:
            video_ids: è§†é¢‘IDåˆ—è¡¨
            
        Returns:
            è§†é¢‘è¯¦æƒ…åˆ—è¡¨
        """
        videos_details = []
        
        # YouTube APIé™åˆ¶æ¯æ¬¡æœ€å¤š50ä¸ªè§†é¢‘
        for i in range(0, len(video_ids), 50):
            batch_ids = video_ids[i:i+50]
            
            try:
                request = self.youtube.videos().list(
                    part="snippet,statistics,contentDetails",
                    id=','.join(batch_ids)
                )
                response = request.execute()
                
                for item in response.get('items', []):
                    video_info = self._parse_video_data(item)
                    videos_details.append(video_info)
                    
            except HttpError as e:
                print(f"âŒ è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥: {e}")
                continue
        
        print(f"âœ… æˆåŠŸè·å– {len(videos_details)} ä¸ªè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯")
        return videos_details
    
    def _parse_video_data(self, item: Dict) -> Dict:
        """è§£æè§†é¢‘æ•°æ®"""
        snippet = item['snippet']
        statistics = item['statistics']
        content_details = item['contentDetails']
        
        # è®¡ç®—å‘å¸ƒå¤©æ•°
        published_at = datetime.strptime(snippet['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')
        days_since_published = max(1, (datetime.now() - published_at).days)
        
        # è·å–æ•°æ®
        view_count = int(statistics.get('viewCount', 0))
        like_count = int(statistics.get('likeCount', 0))
        comment_count = int(statistics.get('commentCount', 0))
        
        # è®¡ç®—äº’åŠ¨ç‡
        engagement_rate = 0
        if view_count > 0:
            engagement_rate = ((like_count + comment_count) / view_count) * 100
        
        # è®¡ç®—çƒ­åº¦æŒ‡æ•°
        heat_score = self._calculate_heat_score(
            view_count, like_count, comment_count, days_since_published
        )
        
        # è§£æè§†é¢‘æ—¶é•¿
        duration = self._parse_duration(content_details['duration'])
        duration_seconds = self._parse_duration_seconds(content_details['duration'])

        # é¢„ä¼°æ”¶ç›Šï¼ˆç¾å…ƒï¼‰
        revenue_low, revenue_high, revenue_mid = self._estimate_revenue(view_count)
        hot_reasons = self._analyze_hot_reasons(
            view_count=view_count,
            like_count=like_count,
            comment_count=comment_count,
            engagement_rate=engagement_rate,
            days_since_published=days_since_published,
            duration_seconds=duration_seconds,
            title=snippet['title']
        )
        trend = self._analyze_trend(
            view_count=view_count,
            engagement_rate=engagement_rate,
            days_since_published=days_since_published,
            duration_seconds=duration_seconds
        )
        
        return {
            'video_id': item['id'],
            'title': snippet['title'],
            'channel_title': snippet['channelTitle'],
            'published_at': published_at.strftime('%Y-%m-%d'),
            'days_since_published': days_since_published,
            'duration': duration,
            'duration_seconds': duration_seconds,
            'view_count': view_count,
            'like_count': like_count,
            'comment_count': comment_count,
            'engagement_rate': round(engagement_rate, 2),
            'heat_score': round(heat_score, 2),
            'revenue_low': revenue_low,
            'revenue_high': revenue_high,
            'revenue_mid': revenue_mid,
            'hot_reasons': hot_reasons,
            'hot_reasons_text': '; '.join(hot_reasons[:4]),
            'avg_daily_views': trend['avg_daily_views'],
            'trend_label': trend['label'],
            'trend_score': trend['score'],
            'trend_points': trend['points'],
            'url': f"https://www.youtube.com/watch?v={item['id']}",
            'thumbnail': snippet['thumbnails']['high']['url'],
            'description': snippet.get('description', '')[:200]  # å‰200å­—ç¬¦
        }
    
    def _calculate_heat_score(self, views: int, likes: int, comments: int, days: int) -> float:
        """
        è®¡ç®—çƒ­åº¦æŒ‡æ•°ï¼ˆé’ˆå¯¹æ¬è¿ä¼˜åŒ–ï¼šæ›´æ³¨é‡äº’åŠ¨ç‡ï¼‰
        
        å…¬å¼ï¼š(æ’­æ”¾é‡ Ã— 0.3 + ç‚¹èµæ•° Ã— 30 + è¯„è®ºæ•° Ã— 15) / å‘å¸ƒå¤©æ•°
        äº’åŠ¨é«˜çš„è§†é¢‘è¯´æ˜å†…å®¹æ›´æœ‰å¸å¼•åŠ›ï¼Œé€‚åˆæ¬è¿
        """
        score = (views * 0.3 + likes * 30 + comments * 15) / max(1, days)
        return score
    
    def _parse_duration(self, duration_str: str) -> str:
        """è§£æISO 8601æ—¶é•¿æ ¼å¼ä¸ºå¯è¯»æ ¼å¼"""
        match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
        if not match:
            return "00:00"
        
        hours, minutes, seconds = match.groups()
        hours = int(hours) if hours else 0
        minutes = int(minutes) if minutes else 0
        seconds = int(seconds) if seconds else 0
        
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
        else:
            return f"{minutes:02d}:{seconds:02d}"
    
    def _parse_duration_seconds(self, duration_str: str) -> int:
        """è§£æISO 8601æ—¶é•¿æ ¼å¼ä¸ºæ€»ç§’æ•°"""
        match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
        if not match:
            return 0
        
        hours, minutes, seconds = match.groups()
        hours = int(hours) if hours else 0
        minutes = int(minutes) if minutes else 0
        seconds = int(seconds) if seconds else 0
        
        return hours * 3600 + minutes * 60 + seconds

    def _estimate_revenue(self, views: int) -> tuple:
        """åŸºäºæ’­æ”¾é‡å’ŒCPMèŒƒå›´é¢„ä¼°æ”¶ç›Šï¼ˆç¾å…ƒï¼‰"""
        if views <= 0:
            return 0.0, 0.0, 0.0

        low = round((views / 1000) * self.cpm_low, 2)
        high = round((views / 1000) * self.cpm_high, 2)
        mid = round((low + high) / 2, 2)
        return low, high, mid

    def _analyze_trend(self,
                       view_count: int,
                       engagement_rate: float,
                       days_since_published: int,
                       duration_seconds: int) -> dict:
        """ç®€å•è¶‹åŠ¿åˆ†æï¼šæ—¥å‡æ’­æ”¾ã€è¶‹åŠ¿è¯„åˆ†ã€æ ‡ç­¾ã€æ›²çº¿ç‚¹"""
        days = max(1, min(days_since_published, 90))
        avg_daily = view_count / days

        score = avg_daily
        score += engagement_rate * 1500  # äº’åŠ¨åŠ æƒ
        if days_since_published <= 7:
            score *= 1.15  # æ–°è§†é¢‘è½»å¾®åŠ æˆ
        if duration_seconds <= 120:
            score *= 1.05  # çŸ­å¿«å†…å®¹å†åŠ æˆ

        # æ ‡ç­¾
        if days_since_published <= 3 and avg_daily >= 50000:
            label = "çˆ†å‘æœŸ"
        elif avg_daily >= 100000:
            label = "é«˜é€Ÿå¢é•¿"
        elif avg_daily >= 30000:
            label = "ç¨³å®šå¢é•¿"
        elif avg_daily >= 10000:
            label = "å¹³ç¨³"
        else:
            label = "ç¼“æ…¢"

        points = self._build_trend_points(view_count, days_since_published)
        return {
            'avg_daily_views': round(avg_daily, 2),
            'score': round(score, 2),
            'label': label,
            'points': points
        }

    def _build_trend_points(self, view_count: int, days_since_published: int) -> list:
        """æ„é€ ä¸€ä¸ªç®€å•çš„è¶‹åŠ¿æ›²çº¿ï¼ˆçº¿æ€§/è¿‘ä¼¼ï¼‰ï¼Œç”¨äºå‰ç«¯å°å‹æŠ˜çº¿å›¾"""
        days = max(1, days_since_published)
        window = max(3, min(self.trend_window_days, days))
        avg = view_count / days
        # ç”Ÿæˆçª—å£å†…çš„ç´¯ç§¯æ›²çº¿ï¼Œå‡è®¾å‰æœŸè¾ƒæ…¢ã€åæœŸåŠ é€Ÿï¼ˆç®€å•äºŒæ¬¡å¢é•¿ï¼‰
        pts = []
        for i in range(window):
            t = (i + 1) / window
            factor = 0.6 + 0.4 * (t ** 1.5)  # æœ«æ®µç¨åŠ é€Ÿ
            pts.append(int(avg * days * factor / window))
        # ç¡®ä¿æœ€åä¸€ç‚¹æ¥è¿‘æ€»æ’­æ”¾
        pts[-1] = view_count
        return pts

    def _analyze_hot_reasons(self,
                             view_count: int,
                             like_count: int,
                             comment_count: int,
                             engagement_rate: float,
                             days_since_published: int,
                             duration_seconds: int,
                             title: str) -> list:
        """åŸºäºç®€å•è§„åˆ™çš„çˆ†çº¢åŸå› åˆ†æï¼ˆæ— LLMï¼Œçº¯å¯å‘å¼ï¼‰"""
        reasons = []

        # äº’åŠ¨å’Œå£ç¢‘
        like_rate = (like_count / view_count * 100) if view_count else 0
        comment_rate = (comment_count / view_count * 100) if view_count else 0
        if engagement_rate >= 4:
            reasons.append("high_engagement")
        elif engagement_rate >= 2.5:
            reasons.append("good_engagement")
        if like_rate >= 2.0:
            reasons.append("high_like_rate")
        if comment_rate >= 0.1:
            reasons.append("high_comment_rate")

        # æ—¶é•¿å’Œå®Œæ’­æ½œåŠ›
        if 240 <= duration_seconds <= 600:
            reasons.append("optimal_duration")
        elif 60 <= duration_seconds < 240:
            reasons.append("short_duration")

        # æ–°é²œåº¦
        if days_since_published <= 7:
            reasons.append("fresh_7d")
        elif days_since_published <= 14:
            reasons.append("fresh_14d")

        # æ ‡é¢˜å…³é”®è¯
        tl = title.lower()
        if any(k in tl for k in self.hot_keywords):
            reasons.append("title_clickbait")

        # åŸºæ•°ä¸ç¤¾äº¤è¯æ˜
        if view_count >= 500000:
            reasons.append("high_views")

        # å…œåº•
        if not reasons:
            reasons.append("general_good")

        return reasons
    
    def filter_videos(self, 
                     videos: List[Dict],
                     min_views: int = 50000,
                     min_engagement: float = 2.0,
                     max_days: int = 14,
                     min_duration: int = 60,
                     max_duration: int = 900) -> List[Dict]:
        """
        ç­›é€‰é€‚åˆæ¬è¿çš„æ¬§ç¾çƒ­é—¨è§†é¢‘
        
        Args:
            videos: è§†é¢‘åˆ—è¡¨
            min_views: æœ€ä½æ’­æ”¾é‡ï¼ˆé»˜è®¤5ä¸‡ï¼Œè¯æ˜æœ‰çƒ­åº¦ä¸”å®¹æ˜“æ¬è¿ï¼‰
            min_engagement: æœ€ä½äº’åŠ¨ç‡(%)ï¼ˆé»˜è®¤2%ï¼‰
            max_days: æœ€å¤šå‘å¸ƒå¤©æ•°ï¼ˆé»˜è®¤14å¤©ï¼Œä¿è¯å†…å®¹æ–°é²œï¼‰
            min_duration: æœ€çŸ­æ—¶é•¿ï¼ˆç§’ï¼Œé»˜è®¤60ç§’ï¼‰
            max_duration: æœ€é•¿æ—¶é•¿ï¼ˆç§’ï¼Œé»˜è®¤900ç§’=15åˆ†é’Ÿï¼Œé€‚åˆçŸ­è§†é¢‘å¹³å°ï¼‰
            
        Returns:
            ç­›é€‰åçš„è§†é¢‘åˆ—è¡¨
        """
        filtered = [
            v for v in videos
            if v['view_count'] >= min_views
            and v['engagement_rate'] >= min_engagement
            and v['days_since_published'] <= max_days
            and min_duration <= v['duration_seconds'] <= max_duration
        ]
        
        # æŒ‰çƒ­åº¦æ’åº
        filtered.sort(key=lambda x: x['heat_score'], reverse=True)
        
        print(f"âœ… ç­›é€‰å‡º {len(filtered)} ä¸ªé€‚åˆæ¬è¿çš„è§†é¢‘")
        print(f"   (æ—¶é•¿: {min_duration//60}-{max_duration//60}åˆ†é’Ÿ, æ’­æ”¾é‡â‰¥{min_views:,}, äº’åŠ¨ç‡â‰¥{min_engagement}%)")
        return filtered
    
    def export_to_excel(self, videos: List[Dict], filename: str = None):
        """
        å¯¼å‡ºåˆ°Excel
        
        Args:
            videos: è§†é¢‘åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        if not videos:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"output/youtube_analysis_{timestamp}.xlsx"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else 'output', exist_ok=True)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(videos)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        column_order = [
            'heat_score', 'title', 'view_count', 'like_count', 'comment_count',
            'engagement_rate', 'revenue_mid', 'revenue_low', 'revenue_high',
            'hot_reasons_text',
            'avg_daily_views', 'trend_label',
            'channel_title', 'published_at', 'days_since_published',
            'duration', 'url', 'video_id'
        ]
        df = df[column_order]
        
        # é‡å‘½ååˆ—ä¸ºä¸­æ–‡
        df.columns = [
            'çƒ­åº¦æŒ‡æ•°', 'è§†é¢‘æ ‡é¢˜', 'æ’­æ”¾é‡', 'ç‚¹èµæ•°', 'è¯„è®ºæ•°',
            'äº’åŠ¨ç‡(%)', 'é¢„ä¼°æ”¶ç›Š(ä¸­å€¼$)', 'é¢„ä¼°æ”¶ç›Š(ä½$)', 'é¢„ä¼°æ”¶ç›Š(é«˜$)',
            'çˆ†çº¢åŸå› ', 'æ—¥å‡æ’­æ”¾', 'è¶‹åŠ¿æ ‡ç­¾',
            'é¢‘é“åç§°', 'å‘å¸ƒæ—¥æœŸ', 'å‘å¸ƒå¤©æ•°',
            'æ—¶é•¿', 'è§†é¢‘é“¾æ¥', 'è§†é¢‘ID'
        ]
        
        # å¯¼å‡ºExcel
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='è§†é¢‘åˆ†æ', index=False)
            
            # è·å–å·¥ä½œè¡¨
            worksheet = writer.sheets['è§†é¢‘åˆ†æ']
            
            # è®¾ç½®åˆ—å®½
            column_widths = {
                'A': 12,  # çƒ­åº¦æŒ‡æ•°
                'B': 50,  # è§†é¢‘æ ‡é¢˜
                'C': 12,  # æ’­æ”¾é‡
                'D': 10,  # ç‚¹èµæ•°
                'E': 10,  # è¯„è®ºæ•°
                'F': 12,  # äº’åŠ¨ç‡
                'G': 20,  # é¢‘é“åç§°
                'H': 12,  # å‘å¸ƒæ—¥æœŸ
                'I': 10,  # å‘å¸ƒå¤©æ•°
                'J': 10,  # æ—¶é•¿
                'K': 40,  # è§†é¢‘é“¾æ¥
                'L': 15,  # è§†é¢‘ID
            }
            
            for col, width in column_widths.items():
                worksheet.column_dimensions[col].width = width
        
        abs_path = os.path.abspath(filename)
        print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {abs_path}")
        return abs_path
    
    def analyze(self, 
                input_type: str,
                input_value: str,
                max_results: int = 50,
                min_views: int = 50000,
                min_engagement: float = 2.0,
                export: bool = True,
                language: Optional[str] = None,
                region: Optional[str] = None) -> List[Dict]:
        """
        å®Œæ•´åˆ†ææµç¨‹
        
        Args:
            input_type: è¾“å…¥ç±»å‹ ('keyword' æˆ– 'channel')
            input_value: æœç´¢å…³é”®è¯æˆ–é¢‘é“URL
            max_results: æœ€å¤šåˆ†æè§†é¢‘æ•°
            min_views: æœ€ä½æ’­æ”¾é‡ç­›é€‰
            min_engagement: æœ€ä½äº’åŠ¨ç‡ç­›é€‰
            export: æ˜¯å¦å¯¼å‡ºExcel
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¬ YouTubeè§†é¢‘çƒ­åº¦åˆ†æå·¥å…·")
        print(f"{'='*60}\n")
        
        # 1. è·å–è§†é¢‘ID
        print(f"ğŸ“º æ­£åœ¨è·å–è§†é¢‘åˆ—è¡¨...")
        if input_type == 'keyword':
            video_ids = self.search_videos(input_value, max_results, language=language, region=region)
        elif input_type == 'channel':
            video_ids = self.get_channel_videos(input_value, max_results)
        else:
            print("âŒ æ— æ•ˆçš„è¾“å…¥ç±»å‹")
            return []
        
        if not video_ids:
            print("âŒ æœªæ‰¾åˆ°è§†é¢‘")
            return []
        
        # 2. è·å–è§†é¢‘è¯¦æƒ…
        print(f"\nğŸ“Š æ­£åœ¨è·å–è§†é¢‘è¯¦ç»†æ•°æ®...")
        videos = self.get_video_details(video_ids)
        
        if not videos:
            print("âŒ è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥")
            return []
        
        # 3. ç­›é€‰é€‚åˆæ¬è¿çš„è§†é¢‘
        print(f"\nğŸ” æ­£åœ¨ç­›é€‰é€‚åˆæ¬è¿çš„è§†é¢‘...")
        print(f"   ç­›é€‰æ¡ä»¶: æ’­æ”¾é‡â‰¥{min_views:,}, äº’åŠ¨ç‡â‰¥{min_engagement}%, 14å¤©å†…å‘å¸ƒ, æ—¶é•¿1-15åˆ†é’Ÿ")
        filtered_videos = self.filter_videos(videos, min_views, min_engagement)
        
        # 4. æ˜¾ç¤ºTop 10
        print(f"\nğŸ† Top 10 çƒ­é—¨è§†é¢‘:")
        print(f"{'-'*60}")
        for i, video in enumerate(filtered_videos[:10], 1):
            print(f"{i}. [{video['heat_score']:.0f}åˆ†] {video['title'][:40]}...")
            print(f"   ğŸ“ˆ {video['view_count']:,}æ’­æ”¾ | ğŸ‘ {video['like_count']:,} | ğŸ’¬ {video['comment_count']:,}")
            print(f"   ğŸ’° é¢„ä¼°æ”¶ç›Š: ${video['revenue_mid']:,} (ä½:${video['revenue_low']:,} - é«˜:${video['revenue_high']:,})")
            print(f"   â­ çˆ†çº¢åŸå› : {', '.join(video.get('hot_reasons', [])[:3])}")
            print(f"   ğŸ“Š è¶‹åŠ¿: {video.get('trend_label')} | æ—¥å‡ {video.get('avg_daily_views'):,} æ’­æ”¾")
            print(f"   ğŸ”— {video['url']}\n")
        
        # 5. å¯¼å‡ºExcel
        if export and filtered_videos:
            print(f"\nğŸ’¾ æ­£åœ¨å¯¼å‡ºæ•°æ®...")
            self.export_to_excel(filtered_videos)
        
        print(f"\n{'='*60}")
        print(f"âœ… åˆ†æå®Œæˆ! å…±æ‰¾åˆ° {len(filtered_videos)} ä¸ªé€‚åˆæ¬è¿çš„æ¬§ç¾çƒ­é—¨è§†é¢‘")
        print(f"ğŸ’¡ æç¤º: è¿™äº›è§†é¢‘åœ¨æ¬§ç¾åœ°åŒºå—æ¬¢è¿ï¼Œæ—¶é•¿é€‚ä¸­ï¼Œé€‚åˆæœ¬åœ°åŒ–åæ¬è¿åˆ°å°çº¢ä¹¦/æŠ–éŸ³")
        print(f"{'='*60}\n")
        
        return filtered_videos


def main():
    """ä¸»ç¨‹åº"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘       YouTubeæ¬§ç¾çƒ­é—¨è§†é¢‘åˆ†æå·¥å…· v2.0                    â•‘
    â•‘    ç­›é€‰é€‚åˆæ¬è¿åˆ°å°çº¢ä¹¦/æŠ–éŸ³çš„æ¬§ç¾åœ°åŒºçƒ­é—¨å†…å®¹            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # è¯»å–APIå¯†é’¥
    api_key = os.getenv('YOUTUBE_API_KEY')
    
    if not api_key:
        print("\nâš ï¸ è¯·å…ˆè®¾ç½®YouTube APIå¯†é’¥!")
        print("\nè·å–APIå¯†é’¥æ­¥éª¤:")
        print("1. è®¿é—® https://console.cloud.google.com/")
        print("2. åˆ›å»ºé¡¹ç›® â†’ å¯ç”¨YouTube Data API v3")
        print("3. åˆ›å»ºå‡­æ® â†’ APIå¯†é’¥")
        print("\nè®¾ç½®æ–¹æ³•:")
        print("Windows: set YOUTUBE_API_KEY=ä½ çš„å¯†é’¥")
        print("æˆ–åœ¨config.jsonä¸­é…ç½®")
        
        # å°è¯•ä»config.jsonè¯»å–
        if os.path.exists('config.json'):
            with open('config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                api_key = config.get('youtube_api_key')
        
        if not api_key:
            return
    
    analyzer = YouTubeAnalyzer(api_key)
    
    # äº¤äº’å¼é€‰æ‹©
    print("\nè¯·é€‰æ‹©åˆ†ææ¨¡å¼:")
    print("1. æŒ‰å…³é”®è¯æœç´¢")
    print("2. åˆ†ææŒ‡å®šé¢‘é“")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1/2): ").strip()
    
    if choice == '1':
        keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ (è‹±æ–‡): ").strip()
        if keyword:
            analyzer.analyze('keyword', keyword, max_results=50)
    elif choice == '2':
        channel_url = input("è¯·è¾“å…¥é¢‘é“URLæˆ–ID: ").strip()
        if channel_url:
            analyzer.analyze('channel', channel_url, max_results=50)
    else:
        print("âŒ æ— æ•ˆçš„é€‰é¡¹")


if __name__ == "__main__":
    main()
